{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21beeddd",
   "metadata": {},
   "source": [
    "# HW2\n",
    "\n",
    "It's highly recommended to write solutions in markdown cells just in this notebook. If you do so, you'll get up to $1$ bonus point depending on how accurate and neat your work is. You can also write the solutions to math problems by hand and then send photos of them, but there will be no bonus in such case. If you are not familiar with LaTeX, see a 2-page [cheat sheet](http://tug.ctan.org/info/undergradmath/undergradmath.pdf) for a quick start.\n",
    "\n",
    "## Task 1.1 (1 point)\n",
    "\n",
    "Let $\\xi \\sim \\mathrm{Bin}(n, p)$. Prove that $\\mathbb E\\xi = np$, $\\mathbb V\\xi = np(1-p)$.\n",
    "\n",
    "### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a5ca88-8e21-4282-a1bc-8ecc7612a856",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "P(\\xi = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "\n",
    "1. \n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\xi) = \\sum_{k=0}^{n} k \\cdot P(\\xi = k)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\xi) = \\sum_{k=0}^{n} k \\binom{n}{k} p^k (1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\xi) = \\sum_{k=0}^{n} k \\binom{n}{k} p^k (1-p)^{n-k} = \\sum_{k=0}^{n} k \\binom{n}{k} p^k (1-p)^{n-k} = \\sum_{k=1}^{n} k \\binom{n}{k} p^k (1-p)^{n-k} \\quad \\text{(The first term with } k = 0 \\text{ contributes 0)}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\xi) = \\sum_{k=1}^{n} \\frac{n!}{k!(n-k)!} k p^k (1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\xi) = \\sum_{k=1}^{n} \\frac{n}{k} \\frac{n!}{k!(n-k)!} k p^k (1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{n}{k} \\frac{n!}{k!(n-k)!} k = \\frac{n}{k} \\cdot \\frac{n!}{k!(n-k)!} \\cdot k = n\\binom{n-1}{k-1}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\xi) = \\sum_{k=1}^{n} n\\binom{n-1}{k-1} p^k (1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\xi) = n\\sum_{k=1}^{n} \\binom{n-1}{k-1} p^k (1-p)^{n-k} = n\\cdot 1^n = n\n",
    "$$\n",
    "\n",
    "\n",
    "2. \n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(\\xi) = \\mathbb{E}(\\xi^2) - \\left(\\mathbb{E}(\\xi)\\right)^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "P(\\xi^2 = k) = \\binom{n}{k} p^{2k} (1-p^2)^{n-k}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\xi^2) = \\sum_{k=0}^{n} k^2 \\binom{n}{k} p^{2k} (1-p^2)^{n-k}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\xi^2) = \\sum_{k=1}^{n} k^2 \\binom{n}{k} p^{2k} (1-p^2)^{n-k}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\xi^2) = \\sum_{k=1}^{n} \\frac{n}{k} \\frac{n!}{k!(n-k)!} k^2 p^{2k} (1-p^2)^{n-k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{n}{k} \\frac{n!}{k!(n-k)!} k^2 = n \\cdot \\binom{n-1}{k-1} \\cdot k = n(k-1)\\binom{n-1}{k-1} + n\\binom{n-1}{k-1}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\xi^2) = \\sum_{k=1}^{n} \\left[n(k-1)\\binom{n-1}{k-1} + n\\binom{n-1}{k-1}\\right] p^{2k} (1-p^2)^{n-k}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\xi^2) = \\sum_{k=1}^{n} n(k-1)\\binom{n-1}{k-1} p^{2k} (1-p^2)^{n-k} + \\sum_{k=1}^{n} n\\binom{n-1}{k-1} p^{2k} (1-p^2)^{n-k}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "n(k-1)\\binom{n-1}{k-1} = n\\binom{n-1}{k} = n\\binom{n-1}{n-k}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^{n} n\\binom{n-1}{n-k} p^{2k} (1-p^2)^{n-k}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^{n} n\\binom{n-1}{k-1} p^{2k} (1-p^2)^{n-k} = \\sum_{k=0}^{n-1} n\\binom{n-1}{k} p^{2(k+1)} (1-p^2)^{n-1-k}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\sum_{k=0}^{n-1} n\\binom{n-1}{k} p^{2(k+1)} (1-p^2)^{n-1-k} = np^2\\sum_{k=0}^{n-1} \\binom{n-1}{k} p^{2k} (1-p^2)^{n-1-k}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\xi^2) = n(p^2 + (1-p^2))^{n-1} = n\n",
    "$$\n",
    "\n",
    "Finally, we can calculate the variance:\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(\\xi) = \\mathbb{E}(\\xi^2) - \\left(\\mathbb{E}(\\xi)\\right)^2 = n - (np)^2 = np(1-p)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffd947",
   "metadata": {},
   "source": [
    "## Task 1.2 (2 points)\n",
    "\n",
    "Find the entropy of gamma distribution $\\mathrm{Gamma}(\\alpha, \\beta)$ whose density is\n",
    "\n",
    "$$\n",
    "    p(x) = \\frac1{\\Gamma(\\alpha) \\beta^\\alpha} x^{\\alpha - 1} e^{-\\frac x \\beta}.\n",
    "$$\n",
    "\n",
    "*Hint*. The answer is expressed in terms of **digamma function** $\\psi(t) = \\frac{\\Gamma'(t)}{\\Gamma(t)}$.\n",
    "\n",
    "### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b45f87-362a-4766-a260-2f1fa994e9f2",
   "metadata": {},
   "source": [
    "To find the entropy of a gamma distribution $$( \\Gamma(\\alpha, \\beta) )$$, we can use the formula for the entropy of a continuous random variable:\n",
    "\n",
    "$$\n",
    "H(X) = -\\int_{-\\infty}^{\\infty} p(x) \\log(p(x)) \\, dx\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{\\Gamma(\\alpha) \\beta^\\alpha} x^{\\alpha - 1} e^{-\\frac{x}{\\beta}}\n",
    "$$\n",
    "\n",
    "We'll use this PDF to calculate the entropy:\n",
    "\n",
    "$$\n",
    "H(X) = -\\int_{0}^{\\infty} \\frac{1}{\\Gamma(\\alpha) \\beta^\\alpha} x^{\\alpha - 1} e^{-\\frac{x}{\\beta}} \\log\\left(\\frac{1}{\\Gamma(\\alpha) \\beta^\\alpha} x^{\\alpha - 1} e^{-\\frac{x}{\\beta}}\\right) \\, dx\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "H(X) = -\\int_{0}^{\\infty} \\frac{1}{\\Gamma(\\alpha) \\beta^\\alpha} x^{\\alpha - 1} e^{-\\frac{x}{\\beta}} \\left(\\log\\left(\\frac{1}{\\Gamma(\\alpha) \\beta^\\alpha}\\right) + \\log\\left(x^{\\alpha - 1}\\right) - \\log\\left(e^{-\\frac{x}{\\beta}}\\right)\\right) \\, dx\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "H(X) = -\\int_{0}^{\\infty} \\frac{1}{\\Gamma(\\alpha) \\beta^\\alpha} x^{\\alpha - 1} e^{-\\frac{x}{\\beta}} \\left(-\\log\\left(\\Gamma(\\alpha) \\beta^\\alpha\\right) + (\\alpha - 1) \\log(x) + \\frac{x}{\\beta}\\right) \\, dx\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "H(X) = \\frac{\\log\\left(\\Gamma(\\alpha) \\beta^\\alpha\\right)}{\\Gamma(\\alpha) \\beta^\\alpha} \\int_{0}^{\\infty} x^{\\alpha - 1} e^{-\\frac{x}{\\beta}} \\, dx - \\int_{0}^{\\infty} \\frac{1}{\\Gamma(\\alpha) \\beta^\\alpha} x^{\\alpha - 1} e^{-\\frac{x}{\\beta}} (\\alpha - 1) \\log(x) \\, dx - \\frac{1}{\\beta} \\int_{0}^{\\infty} \\frac{1}{\\Gamma(\\alpha) \\beta^\\alpha} x^\\alpha e^{-\\frac{x}{\\beta}} \\, dx\n",
    "$$\n",
    "\n",
    "\n",
    "1. The first integral \n",
    "\n",
    "$$\n",
    "\\int_{0}^{\\infty} x^{\\alpha - 1} e^{-\\frac{x}{\\beta}} \\, dx = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\n",
    "$$\n",
    "\n",
    "2. The second integral \n",
    "\n",
    "$$\n",
    "\\int_{0}^{\\infty} x^{\\alpha - 1} e^{-\\frac{x}{\\beta}} (\\alpha - 1) \\log(x) \\, dx = (\\alpha - 1) \\int_{0}^{\\infty} x^{\\alpha - 1} e^{-\\frac{x}{\\beta}} \\log(x) \\, dx = (\\alpha - 1) \\beta^\\alpha \\psi(\\alpha)\n",
    "$$\n",
    "\n",
    "3. The third integral \n",
    "$$\n",
    "\\int_{0}^{\\infty} x^\\alpha e^{-\\frac{x}{\\beta}} \\, dx = \\frac{\\beta^{\\alpha + 1}}{\\Gamma(\\alpha + 1)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "H(X) = \\frac{\\log\\left(\\Gamma(\\alpha) \\beta^\\alpha\\right)}{\\Gamma(\\alpha) \\beta^\\alpha} \\cdot \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} - (\\alpha - 1) \\beta^\\alpha \\psi(\\alpha) - \\frac{1}{\\beta} \\cdot \\frac{\\beta^{\\alpha + 1}}{\\Gamma(\\alpha + 1)}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "H(X) = \\log\\left(\\Gamma(\\alpha) \\beta^\\alpha\\right) - (\\alpha - 1) \\beta^\\alpha \\psi(\\alpha) - \\frac{\\beta}{\\alpha}\n",
    "$$\n",
    "\n",
    "The entropy of the gamma distribution $$( \\Gamma(\\alpha, \\beta)) $$ is given by:\n",
    "\n",
    "$$\n",
    "H(X) = \\log\\left(\\Gamma(\\alpha) \\beta^\\alpha\\right) - (\\alpha - 1) \\beta^\\alpha \\psi(\\alpha) - \\frac{\\beta}{\\alpha}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed78562",
   "metadata": {},
   "source": [
    "## Task 1.3 (3 points)\n",
    "\n",
    "Let $X_1, \\ldots, X_n$ is an i.i.d. sample from $\\mathcal N(0, \\sigma^2)$ and $\\widehat \\sigma = \\frac 1n \\sum\\limits_{i=1}^n |X_i|$.\n",
    "\n",
    "* Find bias, se and MSE of the estimation $\\widehat \\sigma$. Is this estimation unbiased? \n",
    "* If $\\widehat \\sigma$ turned out to be biased, fix it to get an unbiased estimation of $\\sigma$. Find se of this corrected estimation. Is it consistent?\n",
    "\n",
    "### YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d9a318-ce29-4211-8b6d-e41b8f14ed79",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Bias**:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Bias}(\\widehat{\\theta}) = \\mathbb{E}(\\widehat{\\theta}) - \\theta\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(|X_i|) = \\sqrt{\\frac{2}{\\pi}}\\sigma\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\widehat{\\sigma}) = \\mathbb{E}\\left(\\frac{1}{n} \\sum_{i=1}^n |X_i|\\right) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}(|X_i|) = \\frac{1}{n} \\cdot n \\cdot \\sqrt{\\frac{2}{\\pi}}\\sigma = \\sqrt{\\frac{2}{\\pi}}\\sigma\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Bias}(\\widehat{\\sigma}) = \\mathbb{E}(\\widehat{\\sigma}) - \\sigma = \\sqrt{\\frac{2}{\\pi}}\\sigma - \\sigma = \\sigma\\left(\\sqrt{\\frac{2}{\\pi}} - 1\\right)\n",
    "$$\n",
    "\n",
    "2. **Standard Error (SE)**:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{SE}(\\widehat{\\sigma}) = \\frac{\\sigma}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "3. **Mean Squared Error (MSE)**:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{MSE}(\\widehat{\\theta}) = \\mathbb{E}\\left((\\widehat{\\theta} - \\theta)^2\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MSE}(\\widehat{\\sigma}) &= \\mathbb{E}\\left((\\widehat{\\sigma} - \\sigma)^2\\right)\\\\\n",
    "&= \\mathbb{E}\\left(\\left(\\sqrt{\\frac{2}{\\pi}}\\sigma - \\sigma\\right)^2\\right)\\\\\n",
    "&= \\sigma^2\\left(\\frac{2}{\\pi} - 1\\right)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "4. **Unbiased Estimation**:\n",
    "\n",
    "\n",
    "$$\n",
    "\\widehat{\\sigma}_{\\text{unbiased}} = \\frac{1}{\\sqrt{\\frac{2}{\\pi}}} \\widehat{\\sigma} = \\sqrt{\\frac{\\pi}{2}} \\widehat{\\sigma}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\widehat{\\sigma}_{\\text{unbiased}}) = \\sqrt{\\frac{\\pi}{2}}\\sqrt{\\frac{2}{\\pi}}\\sigma = \\sigma\n",
    "$$\n",
    "\n",
    "\n",
    "5. **Standard Error of the Unbiased Estimator**:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{SE}(\\widehat{\\sigma}_{\\text{unbiased}}) = \\frac{\\sigma}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "6. **Consistency**:\n",
    "\n",
    "An estimator is considered consistent if it converges in probability to the true parameter value as the sample size $n$ increases. Both $\\widehat{\\sigma}$ and $\\widehat{\\sigma}_{\\text{unbiased}}$ are consistent estimators of $\\sigma$, as their standard errors decrease as $n$ increases, and they both have expected values equal to $\\sigma$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4126703",
   "metadata": {},
   "source": [
    "## Task 1.4 (3 points)\n",
    "\n",
    "Let $X_1, \\ldots, X_n$ is an i.i.d. sample form $U[0, 2\\theta]$. The parameter $\\theta$ can be estimated as\n",
    "$\\widehat\\theta = \\overline X_n$ (sample average) or $\\tilde\\theta = \\mathrm{med}(X_1, \\ldots, X_n)$ (sample median). According to ML Handbook, both estimations are unbiased, and $\\mathbb V \\widehat\\theta = \\frac{\\theta^2}{3n}$, $\\mathbb V \\tilde\\theta = \\frac{\\theta^2}{n+3}$. Hence, due to the central limit theorem\n",
    "\n",
    "$$\n",
    "    \\widehat\\theta \\approx \\mathcal N\\Big(\\theta, \\frac{\\theta^2}{3n}\\Big), \\quad\n",
    "    \\tilde\\theta \\approx \\mathcal N\\Big(\\theta, \\frac{\\theta^2}{n+3}\\Big), \\quad n \\gg 1.\n",
    "$$\n",
    "\n",
    "In this exercise you are suggested to verify this equalities numerically using NumPy, and plot the results (see example below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44dd061",
   "metadata": {},
   "source": [
    "### Possible result of the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e2651e-d880-4d38-b44e-8d761261dcc5",
   "metadata": {},
   "source": [
    "### i dont get it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbb2e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 17,\n",
    "        'weight' : 'normal'\n",
    "       }\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('axes', titlesize=20)\n",
    "plt.rc('legend', fontsize=18)\n",
    "plt.rc('text.latex', preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "plt.rc('text.latex', preamble=r'\\usepackage[T2A]{fontenc}')\n",
    "plt.rc('text.latex', preamble=r'\\usepackage[russian]{babel}')\n",
    "plt.rc('text.latex', preamble=r'\\usepackage{amsmath}')\n",
    "\n",
    "\n",
    "plot_hists(theta=1.5, n=2000, n_samples=4000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
